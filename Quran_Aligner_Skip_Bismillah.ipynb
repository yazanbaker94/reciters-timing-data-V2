{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Quran Word Aligner (Skip Bismillah Version)\n",
                "Generate word-level timestamps for Quran recitations WITHOUT Bismillah.\n",
                "\n",
                "**Use this when:**\n",
                "- Reciter's audio does NOT have Bismillah at the start of first verses\n",
                "- The original aligner created bad timing because it tried to force Bismillah onto the audio\n",
                "\n",
                "**Instructions:**\n",
                "1. Go to Runtime > Change runtime type > Select GPU (T4)\n",
                "2. Run all cells in order\n",
                "3. Set RECITER, SURAH_START, SURAH_END\n",
                "4. Download the output JSON"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# Mount Google Drive - saves progress permanently!\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "print(\"Google Drive mounted!\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Install dependencies\n",
                "!pip install -q openai-whisper python-Levenshtein tqdm"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Check GPU\n",
                "import torch\n",
                "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Download Quran text\n",
                "import urllib.request\n",
                "print(\"Downloading Quran text...\")\n",
                "urllib.request.urlretrieve(\n",
                "    \"https://tanzil.net/pub/download/index.php?quranType=uthmani&outType=txt-2&agree=true\",\n",
                "    \"quran-uthmani.txt\"\n",
                ")\n",
                "print(\"Done!\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Aligner code WITH BISMILLAH SKIP OPTION\n",
                "import json\n",
                "import os\n",
                "import re\n",
                "from pathlib import Path\n",
                "from dataclasses import dataclass\n",
                "from typing import List, Dict, Tuple\n",
                "from tqdm import tqdm\n",
                "import whisper\n",
                "import Levenshtein\n",
                "\n",
                "@dataclass\n",
                "class WordSegment:\n",
                "    word: str\n",
                "    start_ms: int\n",
                "    end_ms: int\n",
                "\n",
                "@dataclass\n",
                "class AlignedSpan:\n",
                "    index_start: int\n",
                "    index_end: int\n",
                "    start_ms: int\n",
                "    end_ms: int\n",
                "\n",
                "def normalize_arabic(text: str) -> str:\n",
                "    diacritics = re.compile(r'[\\u064B-\\u065F\\u0670]')\n",
                "    text = diacritics.sub('', text)\n",
                "    text = re.sub(r'[إأآا]', 'ا', text)\n",
                "    text = re.sub(r'ة', 'ه', text)\n",
                "    text = re.sub(r'ى', 'ي', text)\n",
                "    text = text.replace('\\u0640', '')\n",
                "    return text.strip()\n",
                "\n",
                "def load_quran_text(quran_file: str, skip_bismillah: bool = False) -> Dict[int, str]:\n",
                "    \"\"\"Load Quran text. If skip_bismillah=True, removes first 4 words (Bismillah) from ayah 1.\"\"\"\n",
                "    quran_text = {}\n",
                "    with open(quran_file, 'r', encoding='utf-8') as f:\n",
                "        for line in f:\n",
                "            line = line.strip()\n",
                "            if not line or line.startswith('#'):\n",
                "                continue\n",
                "            parts = line.split('|')\n",
                "            if len(parts) >= 3:\n",
                "                surah = int(parts[0])\n",
                "                ayah = int(parts[1])\n",
                "                text = parts[2]\n",
                "                \n",
                "                # Skip Bismillah for first ayah (except surah 1 and 9)\n",
                "                if skip_bismillah and ayah == 1 and surah not in [1, 9]:\n",
                "                    words = text.split()\n",
                "                    if len(words) > 4:\n",
                "                        text = ' '.join(words[4:])  # Skip first 4 words (Bismillah)\n",
                "                \n",
                "                key = surah * 1000 + ayah\n",
                "                quran_text[key] = text\n",
                "    return quran_text\n",
                "\n",
                "def align_words(recognized: List[WordSegment], reference_words: List[str]) -> List[AlignedSpan]:\n",
                "    if not recognized or not reference_words:\n",
                "        return []\n",
                "    \n",
                "    rec_normalized = [normalize_arabic(w.word) for w in recognized]\n",
                "    ref_normalized = [normalize_arabic(w) for w in reference_words]\n",
                "    \n",
                "    n, m = len(rec_normalized), len(ref_normalized)\n",
                "    INF = float('inf')\n",
                "    dp = [[INF] * (m + 1) for _ in range(n + 1)]\n",
                "    dp[0][0] = 0\n",
                "    \n",
                "    for j in range(1, m + 1):\n",
                "        dp[0][j] = j * 0.8\n",
                "    for i in range(1, n + 1):\n",
                "        dp[i][0] = i * 1.2\n",
                "    \n",
                "    for i in range(1, n + 1):\n",
                "        for j in range(1, m + 1):\n",
                "            rec_word = rec_normalized[i-1]\n",
                "            ref_word = ref_normalized[j-1]\n",
                "            \n",
                "            if rec_word == ref_word:\n",
                "                cost = 0\n",
                "            else:\n",
                "                ratio = Levenshtein.ratio(rec_word, ref_word)\n",
                "                if ratio > 0.8:\n",
                "                    cost = 0.1\n",
                "                elif ratio > 0.6:\n",
                "                    cost = 0.4\n",
                "                elif ratio > 0.4:\n",
                "                    cost = 0.7\n",
                "                else:\n",
                "                    cost = 1.0\n",
                "            \n",
                "            dp[i][j] = min(\n",
                "                dp[i-1][j-1] + cost,\n",
                "                dp[i-1][j] + 1.2,\n",
                "                dp[i][j-1] + 0.8\n",
                "            )\n",
                "    \n",
                "    alignment = []\n",
                "    i, j = n, m\n",
                "    \n",
                "    while i > 0 or j > 0:\n",
                "        if i > 0 and j > 0:\n",
                "            rec_word = rec_normalized[i-1]\n",
                "            ref_word = ref_normalized[j-1]\n",
                "            \n",
                "            if rec_word == ref_word:\n",
                "                cost = 0\n",
                "            else:\n",
                "                ratio = Levenshtein.ratio(rec_word, ref_word)\n",
                "                if ratio > 0.8:\n",
                "                    cost = 0.1\n",
                "                elif ratio > 0.6:\n",
                "                    cost = 0.4\n",
                "                elif ratio > 0.4:\n",
                "                    cost = 0.7\n",
                "                else:\n",
                "                    cost = 1.0\n",
                "            \n",
                "            if abs(dp[i][j] - (dp[i-1][j-1] + cost)) < 0.001:\n",
                "                alignment.append((i-1, j-1))\n",
                "                i -= 1\n",
                "                j -= 1\n",
                "                continue\n",
                "        \n",
                "        if i > 0 and abs(dp[i][j] - (dp[i-1][j] + 1.2)) < 0.001:\n",
                "            alignment.append((i-1, None))\n",
                "            i -= 1\n",
                "        elif j > 0:\n",
                "            alignment.append((None, j-1))\n",
                "            j -= 1\n",
                "        else:\n",
                "            break\n",
                "    \n",
                "    alignment.reverse()\n",
                "    \n",
                "    spans = []\n",
                "    for rec_idx, aligned_ref_idx in alignment:\n",
                "        if rec_idx is not None and aligned_ref_idx is not None:\n",
                "            spans.append(AlignedSpan(\n",
                "                index_start=aligned_ref_idx,\n",
                "                index_end=aligned_ref_idx + 1,\n",
                "                start_ms=recognized[rec_idx].start_ms,\n",
                "                end_ms=recognized[rec_idx].end_ms\n",
                "            ))\n",
                "    \n",
                "    # Fill gaps\n",
                "    if spans and len(spans) < len(reference_words):\n",
                "        covered_indices = {s.index_start for s in spans}\n",
                "        span_by_idx = {s.index_start: s for s in spans}\n",
                "        filled_spans = []\n",
                "        \n",
                "        for ref_idx in range(len(reference_words)):\n",
                "            if ref_idx in covered_indices:\n",
                "                filled_spans.append(span_by_idx[ref_idx])\n",
                "            else:\n",
                "                prev_span = filled_spans[-1] if filled_spans else None\n",
                "                if prev_span:\n",
                "                    avg_duration = max(prev_span.end_ms - prev_span.start_ms, 300)\n",
                "                    filled_spans.append(AlignedSpan(\n",
                "                        index_start=ref_idx,\n",
                "                        index_end=ref_idx + 1,\n",
                "                        start_ms=prev_span.end_ms,\n",
                "                        end_ms=prev_span.end_ms + avg_duration\n",
                "                    ))\n",
                "        spans = filled_spans\n",
                "    \n",
                "    return spans\n",
                "\n",
                "print(\"Aligner code loaded! (Skip Bismillah version)\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Load Whisper model and Quran text (BISMILLAH SKIPPED)\n",
                "print(\"Loading Whisper large-v3 model...\")\n",
                "model = whisper.load_model(\"large-v3\")\n",
                "print(\"Model loaded!\")\n",
                "\n",
                "# Load Quran text WITH BISMILLAH SKIPPED for first ayahs\n",
                "quran_text = load_quran_text(\"quran-uthmani.txt\", skip_bismillah=True)\n",
                "print(f\"Loaded {len(quran_text)} ayahs (Bismillah skipped for first verses)\")\n",
                "\n",
                "# Verify: Show sample\n",
                "print(f\"\\nSample - Surah 2:1 words: {quran_text[2001].split()[:5]}...\")\n",
                "print(f\"Sample - Surah 10:1 words: {quran_text[10001].split()[:5]}...\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Configuration - EDIT THIS\n",
                "RECITER = \"Abdullah_Basfar_64kbps\"  # Reciter folder name on EveryAyah\n",
                "SURAH_START = 1   # Start surah (1-114)\n",
                "SURAH_END = 114   # End surah (1-114)\n",
                "\n",
                "# Save to Google Drive so progress is never lost!\n",
                "OUTPUT_FILE = \"/content/drive/MyDrive/alignment_skip_bismillah.json\"\n",
                "\n",
                "# Surah ayah counts\n",
                "AYAH_COUNTS = [\n",
                "    7, 286, 200, 176, 120, 165, 206, 75, 129, 109, 123, 111, 43, 52, 99, 128,\n",
                "    111, 110, 98, 135, 112, 78, 118, 64, 77, 227, 93, 88, 69, 60, 34, 30, 73,\n",
                "    54, 45, 83, 182, 88, 75, 85, 54, 53, 89, 59, 37, 35, 38, 29, 18, 45, 60,\n",
                "    49, 62, 55, 78, 96, 29, 22, 24, 13, 14, 11, 11, 18, 12, 12, 30, 52, 52,\n",
                "    44, 28, 28, 20, 56, 40, 31, 50, 40, 46, 42, 29, 19, 36, 25, 22, 17, 19,\n",
                "    26, 30, 20, 15, 21, 11, 8, 8, 19, 5, 8, 8, 11, 11, 8, 3, 9, 5, 4, 7, 3,\n",
                "    6, 3, 5, 4, 5, 6\n",
                "]\n",
                "\n",
                "print(f\"Will process surahs {SURAH_START} to {SURAH_END}\")\n",
                "print(f\"Output file: {OUTPUT_FILE}\")\n",
                "print(\"\\n⚠️ BISMILLAH WILL BE SKIPPED FOR FIRST AYAHS ⚠️\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Download audio and process (with auto-save)\n",
                "import urllib.request\n",
                "import os\n",
                "\n",
                "os.makedirs(\"audio\", exist_ok=True)\n",
                "\n",
                "# Load existing progress if any\n",
                "if os.path.exists(OUTPUT_FILE):\n",
                "    with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:\n",
                "        results = json.load(f)\n",
                "    processed = {(r['surah'], r['ayah']) for r in results}\n",
                "    last_surah = max(r['surah'] for r in results) if results else 0\n",
                "    print(f\"RESUMING - already have {len(results)} ayahs (up to surah {last_surah})\")\n",
                "else:\n",
                "    results = []\n",
                "    processed = set()\n",
                "    print(\"Starting fresh\")\n",
                "\n",
                "save_counter = 0\n",
                "\n",
                "for surah in range(SURAH_START, SURAH_END + 1):\n",
                "    num_ayahs = AYAH_COUNTS[surah - 1]\n",
                "    \n",
                "    # Check if surah already complete\n",
                "    surah_ayahs_done = sum(1 for s, a in processed if s == surah)\n",
                "    if surah_ayahs_done == num_ayahs:\n",
                "        print(f\"Surah {surah} already complete, skipping\")\n",
                "        continue\n",
                "    \n",
                "    print(f\"\\nProcessing Surah {surah} ({num_ayahs} ayahs)...\")\n",
                "    \n",
                "    for ayah in tqdm(range(1, num_ayahs + 1), desc=f\"Surah {surah}\"):\n",
                "        # Skip if already processed\n",
                "        if (surah, ayah) in processed:\n",
                "            continue\n",
                "            \n",
                "        filename = f\"{surah:03d}{ayah:03d}.mp3\"\n",
                "        url = f\"https://everyayah.com/data/{RECITER}/{filename}\"\n",
                "        local_path = f\"audio/{filename}\"\n",
                "        \n",
                "        # Download if not exists\n",
                "        if not os.path.exists(local_path):\n",
                "            try:\n",
                "                urllib.request.urlretrieve(url, local_path)\n",
                "            except Exception as e:\n",
                "                print(f\"Failed to download {filename}: {e}\")\n",
                "                continue\n",
                "        \n",
                "        # Get reference text (BISMILLAH ALREADY SKIPPED)\n",
                "        key = surah * 1000 + ayah\n",
                "        if key not in quran_text:\n",
                "            print(f\"No reference text for {surah}:{ayah}\")\n",
                "            continue\n",
                "        \n",
                "        reference = quran_text[key]\n",
                "        reference_words = reference.split()\n",
                "        \n",
                "        # Transcribe\n",
                "        try:\n",
                "            result = model.transcribe(local_path, language=\"ar\", word_timestamps=True)\n",
                "            \n",
                "            words = []\n",
                "            for segment in result.get(\"segments\", []):\n",
                "                for word_info in segment.get(\"words\", []):\n",
                "                    word = word_info.get(\"word\", \"\").strip()\n",
                "                    if word:\n",
                "                        words.append(WordSegment(\n",
                "                            word=word,\n",
                "                            start_ms=int(word_info[\"start\"] * 1000),\n",
                "                            end_ms=int(word_info[\"end\"] * 1000)\n",
                "                        ))\n",
                "            \n",
                "            spans = align_words(words, reference_words)\n",
                "            \n",
                "            results.append({\n",
                "                \"surah\": surah,\n",
                "                \"ayah\": ayah,\n",
                "                \"segments\": [[s.index_start, s.index_end, s.start_ms, s.end_ms] for s in spans]\n",
                "            })\n",
                "            processed.add((surah, ayah))\n",
                "            save_counter += 1\n",
                "            \n",
                "            # Auto-save every 10 ayahs\n",
                "            if save_counter >= 10:\n",
                "                with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
                "                    json.dump(results, f, ensure_ascii=False)\n",
                "                save_counter = 0\n",
                "                \n",
                "        except Exception as e:\n",
                "            print(f\"Error processing {surah}:{ayah}: {e}\")\n",
                "    \n",
                "    # Also save after each surah completes\n",
                "    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
                "        json.dump(results, f, ensure_ascii=False)\n",
                "    print(f\"Surah {surah} done! Total: {len(results)} ayahs saved\")\n",
                "\n",
                "print(f\"\\n=== COMPLETE! Processed {len(results)} ayahs ===\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Save results\n",
                "with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
                "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
                "\n",
                "print(f\"Saved to {OUTPUT_FILE}\")\n",
                "\n",
                "# Download\n",
                "from google.colab import files\n",
                "files.download(OUTPUT_FILE)"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Notes\n",
                "\n",
                "This version **skips Bismillah** (first 4 words) for the first ayah of each surah,\n",
                "except Surah 1 (Al-Fatiha) and Surah 9 (At-Tawbah).\n",
                "\n",
                "Use this for reciters whose audio does NOT have Bismillah at the start of each surah.\n",
                "\n",
                "### Available Reciters on EveryAyah\n",
                "- `Abdullah_Basfar_64kbps`\n",
                "- `Ghamadi_40kbps`\n",
                "- `Akram_AlAlaqimy_128kbps`\n",
                "- And many more at https://everyayah.com/data/"
            ],
            "metadata": {}
        }
    ]
}